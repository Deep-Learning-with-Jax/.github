## Introduction to deep learning
This fifteen-day crash course is part of the course program for incoming Ph.D. students at the University of Bonn's [BIGS-Neuroscience](https://bigs-neuroscience.de/) and [BIGS Clinical and Population Science](https://bigs-clinpopscience.de/). We are releasing it here for those who could not attend the course in person. Furthermore, we hope that it will help a broader audience.

The material currently consists of lecture videos, slides and exercises.
Most exercises come with unit tests, allowing you to verify your solutions independently. The first exercise explains how to do that.

All exercises will run on Ubuntu 22.04.1 with ffmpeg version 4.4.2.

An extended version of this course is held in person every semester. [Members of the University can register](https://www.hpc.uni-bonn.de/en/training/courses/ml_intro).


Prerequisites:
Programming in Python. If you are unfamiliar with Python, please consult https://docs.python.org/3/tutorial/ before the first session.
University-level math courses make it much easier to understand the material. However, participants from our Humanities departments have completed the course in person.
## Course contents:

### Part 1, Basics
- Day 1: Introduction
    - How to get started
    - [Exercise](https://github.com/Deep-Learning-with-Jax/day_01_exercise_intro)
- Day 2: Optimization
    - The derivative, gradients, optimization via gradient descent.
    - [Exercise](https://github.com/Deep-Learning-with-Jax/day_02_exercise_optimization)   
- Day 3:   Linear Algebra:
   - Matrix multiplication, singular value decomposition, Linear Regression.
   - [Exercise](https://github.com/Deep-Learning-with-Jax/day_03_exercise_algebra)
- Day 4:  Statistics
   - Mean and variance, correlation, Gaussians.
   - [Exercise](https://github.com/Deep-Learning-with-Jax/day_04_exercise_statistics)

### Part 2, Deep Learning
- Day 5: Fully connected networks:
    -  The MNIST-data set, artificial neurons, forward and backward pass.
    -  [Exercise](https://github.com/Deep-Learning-with-Jax/day_05_exercise_neural_networks)
- Day 6: Convolutional neural networks:
    -  The convolution operation and convolutional neural networks.
    -  [Exercise](https://github.com/Deep-Learning-with-Jax/day_06_exercise_cnn)
- Day 7: Optimization for deep neural networks:
    -  Gradient descent with momentum, Adam, early stopping, regularization.
    -  [Exercise](https://github.com/Deep-Learning-with-Jax/day_07_exercise_brain_decode)
- Day 8: Interpretability:
    - Visualization of linear classifiers, saliency maps, integrated gradients
    - [Exercise](https://github.com/Deep-Learning-with-Jax/day_08_exercise_interpretability)
- Day 9: Sequence models:
    - Long-Short-Term-Memory, Gated recurrent units, text-based language models.
    - [Exercise](https://github.com/Deep-Learning-with-Jax/day_09_exercise_sequence_processing)


## Support

We thank the state of North Rhine-Westphalia and the Federal Ministry of Education and Research for supporting this project.

<table>
<tr>
    <td><img src="https://github.com/Machine-Learning-Foundations/.github/blob/main/profile/img/nrw-logo.png" height="150"></td>
    <td><img src="https://github.com/Machine-Learning-Foundations/.github/blob/main/profile/img/BMBF_gefoerdert_2017_en.jpg" height="150"></td>
</tr>
</table>
